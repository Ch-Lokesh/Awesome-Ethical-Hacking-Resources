{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_chat_bot.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ch-Lokesh/Awesome-Ethical-Hacking-Resources/blob/master/tf_chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHPEc2X_HDy8",
        "colab_type": "text"
      },
      "source": [
        "Intent is important for a chat bot, so we make intent classificaion, we make tags, pattern and responses"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLpVByNaG40P",
        "colab_type": "code",
        "outputId": "97da01f3-7332-4f87-bce2-f09d6c8ea80c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "#these are libraries for NLP\n",
        "\n",
        "import nltk #natural language processing tool kit\n",
        "nltk.download('punkt')  #we need to download punkt to tokanise sentences into individual words\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer = LancasterStemmer() #creating an object, stemmming reduces a words into root word(eg cooks,cooking , cooked, cook into cook)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXklsbnWJiVC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ded84a84-0f52-4437-f787-83a8d79d2fe3"
      },
      "source": [
        "#these are libraries for tensorflow\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import tflearn\n",
        "import random\n",
        "import json     #to read json file\n",
        "import pickle   #to save the model\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0808 15:40:27.523344 139970515236736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/summarizer.py:9: The name tf.summary.merge is deprecated. Please use tf.compat.v1.summary.merge instead.\n",
            "\n",
            "W0808 15:40:27.524603 139970515236736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/helpers/trainer.py:25: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "W0808 15:40:27.543280 139970515236736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/collections.py:13: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
            "\n",
            "W0808 15:40:27.550840 139970515236736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:123: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
            "\n",
            "W0808 15:40:27.564546 139970515236736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:129: The name tf.add_to_collection is deprecated. Please use tf.compat.v1.add_to_collection instead.\n",
            "\n",
            "W0808 15:40:27.565218 139970515236736 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/tflearn/config.py:131: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlyrUFQBKVkM",
        "colab_type": "code",
        "outputId": "cae85078-3a38-4272-a78e-345674aaf851",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 111
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0c52c779-4672-49b5-b89a-808f47c792c8\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-0c52c779-4672-49b5-b89a-808f47c792c8\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving intent.json to intent.json\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intent.json': b'{\\n    \"intents\":[\\n        {\"tag\" : \"greetings\",\\n        \"patterns\" :[\"Hi\", \"How are you\", \"Is anyone there?\", \"Hello\", \"Good Day\"],\\n        \"responses\" :[\"Hello\", \"hii\", \"Hey there\", \"Good to See You\", \"How can I help You\"],\\n        \"context_set\" : \"\"\\n        },\\n        {\"tag\" : \"good bye\",\\n        \"patterns\" :[\"Bye\", \"See you later\", \"Text you later\", \"Good bye\"],\\n        \"responses\" : [\"Bye ! Come back again\", \"Have a nice day\", \"See you later\", \"Nice to meet you\"]},\\n        \\n        {\"tag\" : \"thanks\",\\n        \"patterns\" :[\"Thanks\", \"Thank you so much\", \"That\\'s heplful\"],\\n        \"responses\" : [\"Happy to help!\", \"Any time!\", \"My Pleasure\"]},\\n\\n        {\"tag\" : \"location\",\\n        \"patterns\" :[\"where do you live ?\", \"what is your native place ?\", \"what is your home town\"],\\n        \"responses\" :[\"My home town is Warangal\"]},\\n\\n        {\"tag\" : \"your name\",\\n        \"patterns\" : [\"what is your name?\", \"may I know your name ?\", \"what is your good name?\", \"what people call you\",\\n            \"name please\"],\\n        \"responses\" : [\"You are texting to VERONICA version 2.0\", \"This is veronica version 2.0\", \"My name is Veronica\",\\n            \"You can call me Veronica\",\"People call me Veronica, but you can call me senorita\" ]},\\n\\n        {\\n            \"tag\" :\"boss name\",\\n            \"patterns\" : [\"what is your boss name?\", \"who programmed you ?\", \"who is your owner?\", \"may I know about your boss\", \\n            \"tell me about your boss\", \"who made you\"], \\n            \"responses\" :[\"Oh about boss !, His name is Lokesh\", \"I was programmed by lokesh, few of his friends also involved in this\", \\n            \"Don\\'t ask about him, there is nothing great about him to say, If you are still curious, people call him Lokesh\",\\n        \"A lazy guy who is livinig with the name of Lokesh, better stay away from him\"]\\n        }\\n\\n\\n\\n    ]\\n}'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SYGCh86LL4D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing chat bots intent file\n",
        "with open('intent.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM4Q-hl6P3fT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        },
        "outputId": "148ba42d-bbba-4dae-df90-66d8e3a7385f"
      },
      "source": [
        "intents"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'intents': [{'context_set': '',\n",
              "   'patterns': ['Hi', 'How are you', 'Is anyone there?', 'Hello', 'Good Day'],\n",
              "   'responses': ['Hello',\n",
              "    'hii',\n",
              "    'Hey there',\n",
              "    'Good to See You',\n",
              "    'How can I help You'],\n",
              "   'tag': 'greetings'},\n",
              "  {'patterns': ['Bye', 'See you later', 'Text you later', 'Good bye'],\n",
              "   'responses': ['Bye ! Come back again',\n",
              "    'Have a nice day',\n",
              "    'See you later',\n",
              "    'Nice to meet you'],\n",
              "   'tag': 'good bye'},\n",
              "  {'patterns': ['Thanks', 'Thank you so much', \"That's heplful\"],\n",
              "   'responses': ['Happy to help!', 'Any time!', 'My Pleasure'],\n",
              "   'tag': 'thanks'},\n",
              "  {'patterns': ['where do you live ?',\n",
              "    'what is your native place ?',\n",
              "    'what is your home town'],\n",
              "   'responses': ['My home town is Warangal'],\n",
              "   'tag': 'location'},\n",
              "  {'patterns': ['what is your name?',\n",
              "    'may I know your name ?',\n",
              "    'what is your good name?',\n",
              "    'what people call you',\n",
              "    'name please'],\n",
              "   'responses': ['You are texting to VERONICA version 2.0',\n",
              "    'This is veronica version 2.0',\n",
              "    'My name is Veronica',\n",
              "    'You can call me Veronica',\n",
              "    'People call me Veronica, but you can call me senorita'],\n",
              "   'tag': 'your name'},\n",
              "  {'patterns': ['what is your boss name?',\n",
              "    'who programmed you ?',\n",
              "    'who is your owner?',\n",
              "    'may I know about your boss',\n",
              "    'tell me about your boss',\n",
              "    'who made you'],\n",
              "   'responses': ['Oh about boss !, His name is Lokesh',\n",
              "    'I was programmed by lokesh, few of his friends also involved in this',\n",
              "    \"Don't ask about him, there is nothing great about him to say, If you are still curious, people call him Lokesh\",\n",
              "    'A lazy guy who is livinig with the name of Lokesh, better stay away from him'],\n",
              "   'tag': 'boss name'}]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-GtAPImP5sV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?', '.', '!', 'veronica', 'darling', 'baby', 'babe']\n",
        "#loop through each sentence in the intent's patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        #tokanise each and every word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        #adding word to word list\n",
        "        words.extend(w)\n",
        "        #adding word(s) to the documents\n",
        "        documents.append((w, intent['tag']))\n",
        "        #add tags to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hes486U9RJww",
        "colab_type": "code",
        "outputId": "0ec403ce-7ba4-4bad-94f3-a7b07cad7a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "#perform stemming and lower each word as well as remove duplicates\n",
        "words = [stemmer.stem(w.lower())  for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "#removing duplicate classes\n",
        "\n",
        "print(len(documents), \"documents\")\n",
        "print(len(classes), \"classes\", classes)\n",
        "print(len(words), \"unique stemmed words\", words)\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "26 documents\n",
            "6 classes ['greetings', 'good bye', 'thanks', 'location', 'your name', 'boss name']\n",
            "44 unique stemmed words [\"'s\", 'about', 'anyon', 'ar', 'boss', 'bye', 'cal', 'day', 'do', 'good', 'hello', 'hepl', 'hi', 'hom', 'how', 'i', 'is', 'know', 'lat', 'liv', 'mad', 'may', 'me', 'much', 'nam', 'nat', 'own', 'peopl', 'plac', 'pleas', 'program', 'see', 'so', 'tel', 'text', 'thank', 'that', 'ther', 'town', 'what', 'wher', 'who', 'yo', 'you']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ6bkhfoSwpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creaing a training data i.e transforming the documents into tensers\n",
        "training = []\n",
        "output = []\n",
        "\n",
        "#creating an empty array for output\n",
        "output_empty = [0]*len(classes)\n",
        "\n",
        "#creating  a training set, a bag of words for each sentence\n",
        "for doc in documents:\n",
        "    #initialising the bag of words\n",
        "    bag = []\n",
        "    \n",
        "    #list of tokanised words\n",
        "    pattern_words =  doc[0]\n",
        "    #stemming each word\n",
        "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    \n",
        "    #creating bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "        \n",
        "    #output is 1 for current tag and 0 for others\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "    \n",
        "    training.append([bag, output_row])\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxBJENoxOdbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "#creating training lists\n",
        "\n",
        "train_x = list(training[:, 0])    #features\n",
        "train_y = list(training[:, 1])    #labels in the form assosiated tags or intent class\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7JvZjKgPLsO",
        "colab_type": "code",
        "outputId": "f70125ea-f497-4706-d1a4-9eda4ea995e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "source": [
        "# resetting underlying graph data\n",
        "import tensorflow as tf\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#building a neural network\n",
        "net = tflearn.input_data(shape = [None, len(train_x[0])]) #input layer a placeholder will be used a shape or a placeholder must be used\n",
        "net = tflearn.fully_connected(net, 10)    #this and the below are hidden layers\n",
        "net = tflearn.fully_connected(net, 10)\n",
        "net = tflearn.fully_connected(net, len(train_y[0]), activation = 'softmax')   #output layer\n",
        "net = tflearn.regression(net)     #regression linear reduces the loss\n",
        "\n",
        "#defining model and setting up tensorboard\n",
        "model = tflearn.DNN(net, tensorboard_dir = 'tflearn_logs')\n",
        "\n",
        "#start training\n",
        "model.fit(train_x, train_y, n_epoch = 1000, batch_size = 8, show_metric = True)\n",
        "model.save('model.tflearn')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.50477\u001b[0m\u001b[0m | time: 0.014s\n",
            "| Adam | epoch: 1000 | loss: 0.50477 - acc: 0.9311 -- iter: 24/26\n",
            "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.46465\u001b[0m\u001b[0m | time: 0.019s\n",
            "| Adam | epoch: 1000 | loss: 0.46465 - acc: 0.9380 -- iter: 26/26\n",
            "--\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nnLNhHZcSBPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pickle.dump({'words' :words, 'classes' : classes, 'train_x' :train_x, 'train_y':train_y}, open(\"training_data\", \"wb\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UU4ZrUmYTmtR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#restoring all the data structures\n",
        "data = pickle.load(open(\"training_data\", \"rb\"))\n",
        "words = data['words']\n",
        "classes = data['classes']\n",
        "train_x = data['train_x']\n",
        "train_y = data['train_y']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efDJpCRQUrty",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open('intent.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJxvj7u7VIsA",
        "colab_type": "code",
        "outputId": "164e6c34-6f81-48a0-a46b-e56aa1e2ab7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "# load the saved model\n",
        "model.load( './model.tflearn' )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0808 15:41:08.055858 139970515236736 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSA0mmAbVf6m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    #tokanize the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    #stemming each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "#returning bag of words array: 0 or 1 for each word in the bag that exits in sentence\n",
        "\n",
        "def bow(sentence, words, show_details = False):\n",
        "    #tokanize the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    #generating bag of words\n",
        "    bag = [0]*len(words)\n",
        "    \n",
        "    for s in sentence_words:\n",
        "        for i, w in enumerate(words):\n",
        "            if w == s:\n",
        "                bag[i] = 1\n",
        "                if show_details:\n",
        "                    print(\"found in bag :%s\", w)\n",
        "    return (np.array(bag))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4m-6rwlPXP6g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#adding some context to the conversation i.e contextualization, creating a dictionary for user contex\n",
        "\n",
        "context = {}\n",
        "\n",
        "\n",
        "error_threshold = 0.30\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def classify(sentence):\n",
        "    #generate probabilities from the model\n",
        "    results = model.predict([bow(sentence, words)])[0]\n",
        "    \n",
        "    #filter out perdictions below a threshold \n",
        "    results = [[i, r] for i, r in enumerate(results) if r > error_threshold]\n",
        "    #sorting by strength of probability\n",
        "    results.sort(key = lambda x: x[1], reverse = True)\n",
        "    return_list = []\n",
        "    \n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "        \n",
        "    #return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence, userID = '123', show_details = False):\n",
        "    results = classify(sentence)\n",
        "    #if we have a classification then find the matching intent taf\n",
        "    if results:\n",
        "        while results:\n",
        "            #loop as long as there are matches to process\n",
        "            for i in intents['intents']:\n",
        "                #find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    #set contex for this intent if necessary\n",
        "                    if 'context_set' in i:\n",
        "                        if show_details: print('context :', i['context_set'])\n",
        "                        context[userID] = i['context_set']\n",
        "\n",
        "\n",
        "                    #check if this intent is contextual and applies to the user's conversation\n",
        "                    if not 'context_filter' in i or \\\n",
        "                    (userID in context and 'context_filter' in i and i['context_filter'] == context[userID]):\n",
        "                        if show_details : print(\"tag :\", i['tag'])\n",
        "                        #a random response from the intent\n",
        "                        return print(random.choice(i['responses']))\n",
        "        results.pop(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LwfbTO4paUae",
        "colab_type": "code",
        "outputId": "bd3cee6d-f161-4657-d4af-d028aa5c5ec2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classify(\"how are you?\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('greetings', 0.9311854)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QTjSDQdFAc1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        },
        "outputId": "0acdfa9f-82a3-47a5-f93c-2edccfe09512"
      },
      "source": [
        "response(\"Hi darling\", show_details = True)\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "context : \n",
            "tag : greetings\n",
            "Hello\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3-Y4twNJk8x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}